## Running Ollama Third-Party Service on kubernetes

https://opea-project.github.io/latest/getting-started/README.html

### Choosing a Model
You can get the model_id that ollama will launch from the [Ollama Library](https://ollama.com/library).

https://ollama.com/library/llama3.2

on powershell run: ollama run llama3.2